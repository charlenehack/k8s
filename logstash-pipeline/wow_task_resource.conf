input {
  jdbc {
    type => "task_resource" #定义INPUT的type名称,output中可以根据input的type不同,定义不同的输出存储对象
    jdbc_driver_library => "/usr/share/logstash/vendor/mysql-connector-java-5.1.44-bin.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://192.168.1.16:3306/wow_service?useTimezone=true&serverTimezone=UTC&useSSL=false&zeroDateTimeBehavior=convertToNull"  #JDBC链接配置
    jdbc_user => "root"
    jdbc_password => "123456"
    jdbc_default_timezone => "Asia/Shanghai"
    jdbc_page_size => 1000  #logstash每次读取数据条数处理
    jdbc_fetch_size => 1000  #JDBC每次读取加载数据条数
    jdbc_paging_enabled => true
    statement => "SELECT * FROM view_task_resource WHERE updated_at>=date_add(:sql_last_value,INTERVAL -8 HOUR) ORDER BY updated_at ASC"
    use_column_value => true  #根据数据库字段类型自动设置ES字段类型
    tracking_column => "updated_at" #设置根据表的对应字段进行数据更新检查
    tracking_column_type => "timestamp"
    schedule => "* * * * *" #这是默认配置,一分钟执行一次. 修改规则同linux的crontab
    last_run_metadata_path => "/usr/share/logstash/cache/.logstash_wow_task_resource_run" #记录logstash当前type最新更新记录的信息
  }
}

filter {  #数据过来逻辑,可以对要存储到ES的数据进行增删改相关逻辑操作
	if [type] == "task_resource" {
            mutate {
                split => ["resource_tag_ids",","]
            }
        }
}


output {
  if [type] == "task_resource" {    #和INPUT那边定义的type对应,防止数据乱窜
	elasticsearch {
        #action => "update"
	hosts => "192.168.1.11:9200"
	routing => "task_resource"
        index => "wow_task_resource"
        document_id => "%{id}"  # 索引文档的key取值字段
        doc_as_upsert => true  # 文档数据是否执行更新操作,false表示不执行更新,直接删除添加
        document_type => "_doc"  #定义存入ES的索引的文档名称,相当于MYSQL的表名称
    }
  }
}
